## 数据仓库

用来装各种企业数据，并为数据的最终目的地做好准备

## 集群

* 主节点（namenode）
* 从节点（datanode）

## linux简介

### 目录

* /bin：存放系统命令的目录，所有用户都可以执行
* /sbin：保存和系统环境设置有关的命令，只有超级用户可以使用，有些命令可以允许普通用户查看。
* /root：存放root用户的相关文件，root用户的家目录。

### 命令

1. `cd 目录`：**切换操作**。带“/”就是绝对路径。
2. `ls -l`：**展示**当前路径下的所有文件或目录。简写ll。
3. `ls -la`：**隐藏**文件
4. `pwd`：查看**当前路径**
5. `mkdir 目录名`：**创建**目录
6. `mkdir -p 目录名`：创建**递归**目录
7. `touch 文件名`：**创建**文件
8. `rm 文件名`：**删除**文件。加 -f 强制删除
9. `rm -rf 目录名`：**删除**目录。
10. `mv 文件名 目录名`：**移动**文件到目录
11. `mv 名1 名2`：**修改**1名字为2
12. `cp 文件名 目录`：**复制**文件到目录 。 加-r表示递归的文件都复制
13. 编辑命令
14. cat 
14. `ip addr`：查看IP地址
14. `history`：查看历史命令   `history | grep xxx`：模糊查询历史命令
14. `mysql -uroot -p`：用密码登录mysql

## IP地址

分5类：

* 1类：1~127        10.xxx.xxx.xxx（255×255×255）
* 2类：128~191    173.60.xxx.xxx（255×255）
* 3类：
* 4类5类：226~255    国家特殊机构专用

### 配置IP地址

网关前三位+最后一位随机（实习规定110，120，130）

* 查看服务器ip地址：`ip addr`

* 配置路径：`/etc/sysconfig/network-scripts/`

* 修改配置文件：`vim ifcfg-ens33`  （用root用户进行）

  修改内容：BOOTPROTO=static	
  ONBOOT=yes
  IPADDR=192.168.xx.110
  GATEWAY=192.168.xx.2
  NETMASK=255.255.255.0
  DNS1=114.114.114.114
  DNS2=8.8.8.8

### 服务器互传资料

1. 关闭防火墙（内网集群之间不需要加防火墙）

   * `systemctl status firewalld`：查看防火墙
   * `systemctl stop firewalld`：下次开机前关闭防火墙
   * `systemctl disable firewalld`：永久关闭防火墙（只能先临时关闭才能永久关闭）

2. 服务器之间互传资料

   * 普通传输：`scp -r [文件目录][文件名] [ip地址]:[目的地址]`（需要输密码）

   * host映射传输：`scp -r [文件目录][文件名] [主机名]:[目的地址]`（需要建立host映射）

     * 做host映射：

       `vim etc/hosts`：在里面加上[ip地址] [主机名]

   * 免密传输：

     1. 生成各自的私钥和公钥：`ssh-keygen`
     2. 把生成的公钥给别人（也得给自己）：`ssh-copy-id [主机名]`



## 大数据

### 特点

* 大量化：存储量大、增量大
* 多样化：来源多、格式多
* 快速化
* 价值密度低

## Hadoop

### 核心组件

* HDFP（分布式文件系统）：数据存储
  * 主节点：namenode：存储维护元数据
  * 从节点：datanode：存储维护真实数据
* MAPREDUCE（分布式运算编程框架）：数据计算
* YARN（运算资源调度系统）：资源调度（Hadoop框架自主完成
  * 主节点：resourceManager
  * 从节点：nodeManager

### 安装包

* bin里面是启动命令，sbin是集群启动命令

### HDFS上传机制

1. 客户端请求上传文件(分为 n个block块
2. namenode自检(目录里文件是否存在)
3. 返回对应的从节点信息(挑选n个任务比较少的datanode)
4. 请求上传(请求一个最优的节点
5. 连接成功(先上传一个最优的,剩下的互相拷贝)

### Hadoop集群命令

* `hadoop fs +任何命令`

* 上传: `hadoop fs -put [文件名]`

  下载: `hadoop fs -get [/文件名] [路径]`

### namenode工作机制

* **单点故障问题**: 在一个集群中,有且只有一个主节点,当主节点死亡的时候,整个集群就瘫痪掉. 

  **解决办法**: **HA高可用**:一个系统有多个主节点(但是对外服务的只能有一个), 主节点死掉zookeeper集群挑选并唤醒备用机并且告诉备用机之前的交互信息(保存在jounalnode集群里)

* secondaryNamenode: 每隔一短时间将fsimage镜像文件和edits log日志文件整合成新的镜像文件再覆盖原来的fsimage镜像文件（默认休眠）

### HDFS的API调用（使用java代码完成对hdfs的操作）

* **Maven**：集成架包 



## sqoop

* 导入mysql（整表导入）

  ```mysql
  mysql==>hdfs
  sqoop import \
  --connect jdbc:mysql://192.168.211.110:3306/mall \
  --username root \
  --password tang7zaici \
  --table order_info \
  --target-dir hdfs://192.168.211.110:9000/test1 \
  --delete-target-dir \
  --m 1 \
  --fields-terminated-by ","  //数据以逗号切分
  ```

* 按需导入（可以写sql语句）

  ```mysql
  sqoop import \
  --connect jdbc:mysql://192.168.211.110:3306/mall \
  --username root \
  --password tang7zaici \
  --target-dir hdfs://192.168.211.110:9000/test3 \
  --delete-target-dir \
  --m 1 \
  --fields-terminated-by "," \   //数据以逗号切分
  --query 'select id,consignee,consignee_tel,total_amount from order_info where total_amount>=500 and $CONDITIONS;'
  ```

* 导出【hdfs(需求处理之后的数据)=>mysql】

  ```mysql
  sqoop export \
  --connect jdbc:mysql://192.168.211.110:3306/mall \
  --username root \
  --password tang7zaici \
  --export-dir hdfs://192.168.211.110:9000/test3/p* \
  --table my_test1 \
  --m 1 \
  --input-fields-terminated-by ","
  ```

  * 要先建个表

### 封装sqoop脚本

```mysql
#! /bin/bash     //开始标志
APP=mall
sqoop=/opt/module/sqoop-1.4.6/bin/sqoop

if [ -n "$2" ] ;then     //-n判断是否为空，$2表示第二个参数
  do_date=$2      //取时间参数
else
  do_date=`date -d '-1 day' +%F`    //取前一天日期
fi

import_data(){
$sqoop import \
--connect jdbc:mysql://192.168.211.110:3306/mall \
--username root \
--password 123456 \
--target-dir /basic_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 and \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by ',' \
--null-string '\\N' \
--null-non-string '\\N'
}

#导入order_info
import_order_info(){
 import_data order_info "select *
            from order_info
            where date_format(create_time,'%Y-%m-%d')='$do_date'"    //导入时分秒 

}

import_payment_info(){
 import_data payment_info "select
             *
            from payment_info
            where date_format(payment_time,'%Y-%m-%d')='$do_date'"
}

case $1 in    //$1指执行了哪个表
 "order_info")
   import_order_info
;;
 "payment_info")
   import_payment_info
;;
"all")
  import_order_info
  import_payment_info
;;
esac
```



## MapReduce海量数据计算框架（Hadoop的框架之一）

### 功能（词频统计）

* map阶段
  * 给每个数据赋key值
* reduce阶段
  * 根据key值分组，key值相同的分为一组

## hive

不存储数据，数据由hdfs维护

不计算数据，由mapreduce计算

负责把类sql语言转化成mapreduce的代码交给MR框架去执行

### hive建表（内部表）

```mysql
create table 表名(列1 类型,列2 类型,...)
row format delimited fields terminated by ','  //数据以逗号切分
```

### hive插入数据

```mysql
load data inpath '地址' into table 表名;
```

### 外部表（删除表时，内部数据会被保留）

```mysql
create external table 表名(列1 类型,列2 类型,...)
row format delimited fields terminated by ','  //数据以逗号切分
location '/路径';    

//表建好数据会自动取进去
```

### 分区表（针对大量数据）

一般以天分区，查询高效

```mysql
create external table 表名(列1 类型,列2 类型,...)
partitioned by(dt string)  //分区参数
row format delimited fields terminated by ','  //数据以逗号切分
location '/路径';

//分区表需要load数据
load data inpath '/路径' into table 表名 PARTITION(dt="区");
```

### 处理数据

1. 处理array类型数据

   ```mysql
   //电影主演有多位，为数组类型
   
   create table t_movie(mv_name string,actors Array<String>,times data)
   row format delimited fields terminated by ','
   collection items terminated by ':';    //遇到数组类型用:切分, //collection是array的父类
   ```

2. 处理map类型

   ```mysql
   //[家庭成员＋姓名]的数组，map类型
   
   create table t_family(id string,name string, fam_numbers Map<String,String>,age int)
   row format delimited fields terminated by ','
   collection items terminated by '#';
   Map keys terminated by ':';
   
   //拿到所有父亲的名字
   select family_mambers['father'] from t_family;
   ```
   
3. struct类型数据

   ```sql
   create table Struct(id string,name string,usr_infos Struct<age:int,sex:String,address:String>)
   row format delimited fields terminated by ','
   collection items terminated by ':';
   ```

4. json类型数据

   ```sql
   //建表
   create table tmp_json(json String);
   
   //加载数据
   load data inpath '/路径' into table 表名；
   
   //解析json数据
   create table 表名 as
   select 
   get_json_object(json,'$.字段名') as 新表名
   from 表名
   ```
   
   

* 函数：
  * `size(数组)`：数组长度
* 语法：
  * `if(判断语句，"true的执行体"，"false的执行体")`

### hive常用函数

1. 类型转化函数cast
   select * from where cast(rate as int)>=3;

2. 数学函数

   * 向上取整 3.45  3.95 3.01
     ceil

   * 向下取整3.45  3.95 3.01
     floor

   * 四舍五入  取末尾后俩位
     round(3.1415,末尾保留几位)

   * 取绝对值
     abs

3. 字符串函数

   * 截取字符串：substring 简写成 substr  下标从1开始
     select substr("abcdef",下标,个数);

   * 拼接函数
     concat()
     select userid,username,age,concat("省",address) from p_stu;
     concat_ws(",","xiaoming",20,"nan");

   * 切分函数
     split("hadoop,hive,hadoop,hadoop",",")

   * 转大写
     upper

   * 转小写
     lower

4. 时间函数 

   * 时间戳==>时间
     form_unixtime(1324325365,"yyyy-MM-dd")

   * 时间==>时间戳

5. 条件控制函数
   select id,name,
   (CASE
   when speend>=0 and speend<=60 then '低速'
   when speend>60 and speend<=120 then '中速'
   when speend>120 and speend<=180 then '高速'
   else "超速")aaa
   from XXX
   group by aaa

6. if语句
   if(判断体,"代码为true的代码执行体","代码为false的代码执行体")

7. 行转列函数
   数据如下：
   1,zhangsan,化学:物理:数学:语文
   2,lisi,化学:数学:生物:生理:卫生
   3,wangwu,化学:语文:英语:体育:生物
   4,zhaoliu,数学:物理:化学:英语

   * 行转列的需求：
     1,zhangsan,化学
     1,zhangsan,物理
     ....

   * 建表：
     create table t_subjects(userId string,username string,subjects Array<String>)
     row format delimited fields terminated by ','
     collection items terminated by ':';

   * 加载数据：
     load data local inpath '/root/hive_data/subjects.txt' into table t_subjects;

   * 行转列函数 lateral view
     select userId,username,tmp.subject
     from t_subjects lateral view explode(subjects)tmp as subject;

8. 分析函数（打标记函数）

   * 建表：
     create table t_Topn(id string,age int,name string, sex String)
     row format delimited fields terminated by ',';

   * 加载数据：
     load data local inpath '/root/hive_data/topn.txt' into table t_Topn;
     row_number()over(partition by sex order by age desc)
     1,18,a,male  5
     2,19,b,male  4
     7,32,g,male  1
     5,30,e,male  2
     9,30,j,male  3

     3,22,c,female 4
     4,16,d,female 5
     6,26,f,female 3
     8,36,h,female 2
     10,46,k,female 1

     

     需要查询出每种性别中年龄最大的2条数据
     select * from 
     (select id,age,name,sex,
     row_number()over(partition by sex order by age desc)flag 
     from t_Topn)as tmp
     where tmp.flag<=2;

### 需求结果保存在hive中

```sql
create external table farm_result1
(province string,
counts int
)row format delimited fields terminated by ','
location '/hive_data/farmResult/result1';
```

### 加载数据

```sql
insert overwrite table farm_result1
select province,count(distinct market)counts
from t_farm
group by province
having province <> "";
```

### hive结果导入到mysql中

```sql
sqoop export \
--connect 'jdbc:mysql://192.168.70.110:3306/farm?characterEncoding=utf-8' \
--username root \
--password 123456 \
--export-dir hdfs://192.168.70.110:9000/hive_data/farmResult/result1/* \
--table result1 \
--m 1 \
--input-fields-terminated-by ","
```



## spark（一种基于内存的快速、通用、可扩展的大数据分析计算引擎）Scala语言开发

### 三大组件

* sparkCore离线（核心）：基于RDD处理数据   类似于mapReduce
  * RDD：弹性的分布式数据集
* sparkSql离线：基于类SQL语句    类似于hive
* sparkStreaming：实时流

### Scala语言

* `object`：对象
* `def`：定义方法
* `unit`：返回值（return可以省略不写，返回值直接写在最后一行，多个值里取一个用`A._n`）
* 变量：
  * `var`：可变变量
  * `val`：不可变变量



































































## 交付材料

1. [平时作业]->[姓名]->[第1次作业-姓名]（13个word）（阿拉伯数字）

2. [答辩ppt]

3. [数据库脚本]

   * 业务数据库脚本

   * 需求结果数据库脚本
     * 脚本导出：选中数据库->工具->备份->导出

4. [项目源代码]

   * [数据存储]：找的数据、画的图

   * [数据分析（hive/ ）]：idea项目直接拷贝

   * [数据可视化（echarts/supersite）]：[数据可视化项目代码]（打成包）

5. [项目需求说明书]：背景、意义、需求设计（数据是啥，字段是啥，需求是啥）、架构（+图）、数据库设计（+mysql表、图）、项目实现（+截图）、参考文献

6. [数据字典]：他会发

### 22号答辩结束前交
